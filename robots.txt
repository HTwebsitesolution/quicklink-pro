# Robots.txt for QuickLink Pro
# This file tells search engines what they can crawl

User-agent: *
Allow: /

# Allow all pages to be crawled
Allow: /index.html
Allow: /*.css
Allow: /*.js

# Disallow API endpoints from being crawled
Disallow: /api/

# Disallow admin pages if any
Disallow: /admin/

# Allow social media crawlers
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

# Sitemap location
Sitemap: https://quicklinkpro.io/sitemap.xml

# Crawl delay (optional)
Crawl-delay: 1
